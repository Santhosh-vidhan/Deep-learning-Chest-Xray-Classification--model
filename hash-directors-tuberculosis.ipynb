{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(filename))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade tensorflow_hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \nprint(tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH_1 = KaggleDatasets().get_gcs_path('Tuberculosis (TB) Chest X-ray Database')\nGCS_PATH_2 = KaggleDatasets().get_gcs_path('Tuberculosis Image Dataset')\nGCS_PATH_3 = KaggleDatasets().get_gcs_path('TBX 11')\nGCS_PATH_M= KaggleDatasets().get_gcs_path('data-model-2')\nBATCH_SIZE = 30 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [180, 180]\nEPOCHS = 35\nprint(BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames= tf.io.gfile.glob(str(GCS_PATH_1 + '/chest_xray/train/PNEUMONIA/*'))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH_1 + '/chest_xray/val/*/*')))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH_1 + '/chest_xray/test/*/*')))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH_2 + '/COVID-19_Radiography_Dataset/Normal/*')))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH_2 + '/COVID-19_Radiography_Dataset/Viral Pneumonia/*')))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH_3 + '/covid19-xray-dataset-train-test-sets/xray_dataset_covid19/train/*/*')))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH_4 + '/covid19-pneumonia-normal-chest-xray-pa-dataset/normal/*')))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH_4 + '/covid19-pneumonia-normal-chest-xray-pa-dataset/pneumonia/*')))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH_5 + '/train/opacity/*')))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH_5 + '/test/opacity/*')))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH_5 + '/val/opacity/*')))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_base,test_base=train_test_split(filenames,test_size=0.1)\ntrain_base_1,train_base_2=train_test_split(train_base,test_size=0.5)\ntest_1,test_2=train_test_split(test_base,test_size=0.5)\ntrain_1,val_1=train_test_split(train_base_1,test_size=0.2)\ntrain_2,val_2=train_test_split(train_base_2,test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_normal_pneu(file):\n    print(\"\\n\\tTotal images count in training set : \" + str(len(file)))\n    COUNT_NORMAL = len([filename for filename in file if ((\"NORMAL\" in filename) or (\"Normal\" in filename) or (\"normal\" in filename))])\n    print(\"\\n\\tNormal images count in training set : \" + str(COUNT_NORMAL))\n\n    COUNT_PNEUMONIA = len([filename for filename in file if ((\"PNEUMONIA\" in filename) or (\"Viral Pneumonia\" in filename) or (\"pneumonia\" in filename) or (\"opacity\" in filename))])\n    print(\"\\tPneumonia images count in training set : \" + str(COUNT_PNEUMONIA))\n    \n    if (len(file)==(COUNT_NORMAL+COUNT_PNEUMONIA)):\n        print(\"\\n\\tThe function classified the data correctly\")\n    else:\n        print(\"\\n\\tThe function classification is wrong\")\n    return COUNT_NORMAL,COUNT_PNEUMONIA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\nTRAIN_1_DETAILS :\")\nCOUNT_NORMAL,COUNT_PNEUMONIA=count_normal_pneu(train_1)\nprint(\"\\nVAL_1_DETAILS :\")\ncount_normal_pneu(val_1)\nprint(\"\\nTRAIN_2_DETAILS :\")\ncount_normal_pneu(train_2)\nprint(\"\\nVAL_2_DETAILS :\")\ncount_normal_pneu(val_2)\nprint(\"\\nTEST_1_DETAILS :\")\ncount_normal_pneu(test_1)\nprint(\"\\nTEST_2_DETAILS :\")\ncount_normal_pneu(test_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def file_info(file):\n    list_ds = tf.data.Dataset.from_tensor_slices(file)\n    IMG_COUNT = tf.data.experimental.cardinality(list_ds).numpy()\n    print(\"Images count: \" + str(IMG_COUNT),\"\\n\")\n    \n    return list_ds,IMG_COUNT\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_list_ds,TRAIN_IMG_COUNT= file_info(train_1)\nval_list_ds,VAL_IMG_COUNT=file_info(val_1)\ntest_list_ds,TEST_IMG_COUNT=file_info(test_1)\n\nfor f in train_list_ds.take(5):\n    print(f.numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_list_ds_2,TRAIN_IMG_COUNT_2= file_info(train_2)\nval_list_ds_2,VAL_IMG_COUNT_2=file_info(val_2)\ntest_list_ds,TEST_IMG_COUNT=file_info(test_1)\n\nfor f in train_list_ds.take(5):\n    print(f.numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_label(file_path):\n\n    parts = tf.strings.split(file_path, os.path.sep)\n    if ((parts[-2]== \"PNEUMONIA\") or (parts[-2]==\"pneumonia\") or (parts[-2]==\"opacity\") or (parts[-2]==\"Viral Pneumonia\")):\n        return True\n    elif ((parts[-2]== \"Normal\") or (parts[-2]==\"NORMAL\") or (parts[-2]==\"normal\")):\n        return False\n    else:\n        return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_img(img):\n    img=tf.image.decode_image(img, channels=3,expand_animations=False)\n    img=tf.image.convert_image_dtype(img,tf.float32)\n    return tf.image.resize(img, IMAGE_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_path(file_path):\n    label = get_label(file_path)\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n\nval_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n\ntest_ds = test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds_2 = train_list_ds_2.map(process_path, num_parallel_calls=AUTOTUNE)\n\nval_ds_2 = val_list_ds_2.map(process_path, num_parallel_calls=AUTOTUNE)\n\ntest_ds_2 = test_list_ds_2.map(process_path, num_parallel_calls=AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_for_Data_Aug(ds, cache=False,repeat=False,shuffle=False, shuffle_buffer_size=1000):\n\n    if cache: # only for training and validation data set\n        if isinstance(cache, str):\n            ds = ds.cache(cache)\n        else:\n            ds = ds.cache()\n            \n    if shuffle: # only for training and validation data set\n        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n    \n    normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\n    ds = ds.map(lambda x, y: (normalization_layer(x), y))\n    \n    if repeat: # only for training and validation data set\n        ds = ds.repeat()\n\n    ds = ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n\n\n    return ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = prepare_for_Data_Aug(train_ds,cache=True,repeat=True,shuffle=True,shuffle_buffer_size=(TRAIN_IMG_COUNT//4))\n\nval_ds = prepare_for_Data_Aug(val_ds,cache=True,repeat=True,shuffle=True,shuffle_buffer_size=(VAL_IMG_COUNT//4))\n\ntest_ds= prepare_for_Data_Aug(test_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds_2 = prepare_for_Data_Aug(train_ds_2,cache=True,repeat=True,shuffle=True,shuffle_buffer_size=(TRAIN_IMG_COUNT_2//4))\n\nval_ds_2 = prepare_for_Data_Aug(val_ds_2,cache=True,repeat=True,shuffle=True,shuffle_buffer_size=(VAL_IMG_COUNT_2//4))\n\ntest_ds= prepare_for_Data_Aug(test_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv_block(filters):\n    \n    block = tf.keras.Sequential([\n        \n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPool2D()\n        \n    ])\n    \n    return block","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dense_block(units, dropout_rate):\n    \n    block = tf.keras.Sequential([\n        \n        tf.keras.layers.Dense(units, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(dropout_rate)\n        \n    ])\n    \n    return block","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_aug():\n    \n    block = tf.keras.Sequential([\n\n            tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\",),\n            tf.keras.layers.experimental.preprocessing.RandomFlip(\"vertical\",),\n\n    ])\n    \n    return block","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    model = tf.keras.Sequential([\n        \n        tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n        \n        data_aug(),\n        \n        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n        tf.keras.layers.MaxPool2D(),\n        \n        conv_block(32),\n        conv_block(64),\n        \n        conv_block(128),\n        tf.keras.layers.Dropout(0.2),\n        \n        conv_block(256),\n        tf.keras.layers.Dropout(0.2),\n        \n        tf.keras.layers.Flatten(),\n        dense_block(512, 0.7),\n        dense_block(128, 0.5),\n        dense_block(64, 0.3),\n        \n        tf.keras.layers.Dense(1, activation='sigmoid')\n        \n    ])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    \n    model_2 = build_model()\n\n    METRICS = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n    ]\n    \n    model_2.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n        metrics=METRICS\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"xray_model.h5\",\n                                                    save_best_only=True)\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n                                                     restore_best_weights=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1 **(epoch / s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(0.01, 20)\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = model_2.fit(\n    train_ds,\n    steps_per_epoch=int(np.ceil(TRAIN_IMG_COUNT // float(BATCH_SIZE))),\n    epochs=EPOCHS,\n    validation_data=val_ds,\n    validation_steps=int(np.ceil(VAL_IMG_COUNT // float(BATCH_SIZE))),\n    callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.save('./Vidhan_tf',save_format='tf')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reloaded_model = tf.keras.models.load_model('./Vidhan_tf')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = reloaded_model.fit(\n    train_ds_2,\n    steps_per_epoch=int(np.ceil(TRAIN_IMG_COUNT_2 // float(BATCH_SIZE))),\n    epochs=EPOCHS,\n    validation_data=val_ds_2,\n    validation_steps=int(np.ceil(VAL_IMG_COUNT_2 // float(BATCH_SIZE))),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}